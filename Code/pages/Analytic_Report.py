# Importing necessary libraries
import streamlit as st
import google.generativeai as genai
import time
import logging
import functools
import traceback

# Retry decorator with exponential backoff
# This decorator will used with a function for re-executing the function if any error is raised
def retry(retry_num, retry_sleep_sec):
    """
    retry help decorator.
    :param retry_num: the retry num; retry sleep sec
    :return: decorator
    """
    def decorator(func):
        """decorator"""
        # preserve information about the original function, or the func name will be "wrapper" not "func"
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            """wrapper"""
            for attempt in range(retry_num):
                try:
                    return func(*args, **kwargs)  # should return the raw function's return value
                except Exception as err:   # pylint: disable=broad-except
                    logging.error(err)
                    logging.error(traceback.format_exc())
                    time.sleep(retry_sleep_sec)
                logging.error("Trying attempt %s of %s.", attempt + 1, retry_num)
            logging.error("func %s retry failed", func)
            raise Exception('Exceed max retry num: {} failed'.format(retry_num))

        return wrapper

    return decorator




# Setting page title
st.title("Analytics Report")

# Check if the session state has already been initialized
if "message_list" not in st.session_state:
        # Initialize the session state
        st.session_state["message_list"] = []

# Check if the session state has already been initialized
if "save_instruction" not in st.session_state:
        # Initialize the session state
        st.session_state["save_instruction"] = []

# Check if the session state has already been initialized
if "save_data" not in st.session_state:
        # Initialize the session state
        st.session_state["save_data"] = []

# Storing the dataframe from session into variable
df = st.session_state.df

# Storing the query/instruction from Home Page into a variable
if st.session_state["message_list"] != []:
    instruction = st.session_state["message_list"][-1]

# Gen AI model config
genai.configure(api_key=st.secrets["api"])

# Function to create prompt
@st.cache_data
def prompt_maker(dataframe,text):
    """
    Create prompt to generate python function code to extract result data according to dataframe and query/instruction.

    dataframe: dataframe from which result is to be extracted
    text: query/instruction provided by user
    p: prompt text
    """
    p = '\nTable description: '

    for i in list(dataframe.columns.values):
        d = df[i].dtypes
        if d == 'O':
            p += '\n\nColumn ' + i +  ' has following categories \n'
            for j in list(df[i].unique()):
                if type(j) == float:
                    pass 
                else:
                    p += str(j) + ' ,'

        else:
            p += '\n\nColumn ' + i +  ' has ' + str(d) + ' datatype'
    
    p += '\n\nBy using the table description, create a python code function which takes dataframe as input and returns all the records of dataframe which can answer the Question. Keep the name of function short. Don\'t use reset index. Import pandas library in function code. Only use the categories from above Table description for function code. \n '
    p += """\nQuestion : """ + text + '\n'
    return p

# Defining parameters for Gen AI model
parameters = {
    "temperature": 1,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 1500,
    "response_mime_type": "text/plain",
}



# Loading the Gen AI model
@st.cache_resource
def load_models():
    llm = genai.GenerativeModel(model_name="models/gemini-1.5-flash", generation_config=parameters)
    return llm

# Function for formatting the code generated by Gen AI model
@st.cache_data
def func(value):
    """
    Formats generated code for execution. Returns formatted code and function name.

    value: generated code 
    var: function name
    """
    # Remove specific strings from generated code
    if '`' in value:
        value=value[:value.rindex('`')+1]
        value=value.replace("`", "")
    if 'python' in value:
        value=value.replace("python", "")
    
    value=value.replace("Answer:", "")

    # Extracting function name
    var = value[value.index('def')+3:value.index(':')]
    return value , var.strip()

# Function to create prompt
@st.cache_data
def prompt_analytica(dataframe,text):
    """
    Create prompt to generate answer to query by using data

    dataframe: result data
    text: query/instruction provided by user
    p: prompt text
    """
    p = '\nData: \n'

    p += str(dataframe)
    
    p += """\n\nBy using the Data provided above, answer the following Question. Answer only in text.\n\nQuestion : """ + text + '\n'

    return p

# Function to create prompt
@st.cache_data
def prompt_viz(dataframe,text):
    """
    create prompt to suggest chart type based on data and query

    dataframe: result data
    text: query/instruction provided by user
    p: prompt text
    """
    p = '\nData: \n'

    p += str(dataframe)
    
    p += """\n\nBy using the Data provided above, suggest a chart type to answer the following question. Suggest only from area chart, bar chart, line chart, scatter chart and tablular chart. If data is large give output as tablular chart.  Answer only chart type.\n\nquestion : """ + text + '\n'

    return p

if "message_list" in st.session_state and "df" in st.session_state and st.session_state["message_list"] != []:

    llm = load_models()  # Load model

    p = prompt_maker(df,instruction) # create prompt

    # Function to generate code. Stored in cache. Re-execute function if error is raised while executing generated code
    @st.cache_data
    @retry(10, 1)
    def format_code(p):
        code_text = llm.generate_content(p)
        code, var = func(code_text.text)
        exec(code)
        data = eval(var)
        return code, var

    code, var = format_code(p)

    # Function to execute generated code to extract result data and create promot using it.
    @st.cache_data
    def run_code(code,var,instruction):
        exec(code)
        
        data = eval(var)
        analytics_prompt = prompt_analytica(data, instruction)
        return analytics_prompt ,data

    analytics_prompt ,data= run_code(code,var,instruction) # create prompt 
    chat_text = llm.generate_content(analytics_prompt) # generate answers
    chat= chat_text.text   # storing text from model response

    viz = prompt_viz(data,instruction) # create prompt
    chart = llm.generate_content(viz) # storing chart type suggestion from Gen AI model in a variable

    # print latest message
    # initialize chat history
    #if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": chat}]
    
    # Displaying query in UI
    st.write('Your Query is : ' + instruction)
    #'-----Promt for generating code---'
    #p
    # Displaying result data in UI
    #'-----Generated code------'
    #code
    st.write('Result Data :')
    st.dataframe(data)
    # Display chart as per suggestion by Gen AI model
    
    st.write('Visualize Data :')
    if chart.text.strip().lower() == 'area chart':
        st.area_chart(data)
    if chart.text.strip().lower() == 'bar chart':
        st.bar_chart(data)
    if chart.text.strip().lower() == 'line chart':
        st.line_chart(data)
    if chart.text.strip().lower() == 'scatter chart':
        st.scatter_chart(data)
    
    #'------Prompt for generating answers------'
    #analytics_prompt

    # Function to create prompt for chatbot
    @st.cache_data
    def convo_prompt(sess):
        p = ' \nYou are an assistant. Complete the following conversation for assistant only.: \n'

        for message in sess:
            p += '\n' + message["role"] + ' : ' + message["content"] + '\n'
        
        p += '\n' + 'assistant' + ' : '

        return p
    
    
    # display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if prompt := st.chat_input("Have a question about the analysis ?"):
        # add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.spinner("Thinking...."):
            # display assistance response in chat message container
            
            convo = convo_prompt(st.session_state.messages)
            response = llm.generate_content(convo)
            res = response.text
            with st.chat_message("assistant"):
                st.markdown(res)
            # add assistance response to chat history
            st.session_state.messages.append({"role": "assistant", "content": res})


else:
    st.markdown("### Please provide your Query to Generate Insights") 
    
